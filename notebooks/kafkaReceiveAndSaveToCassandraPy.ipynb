{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaReceiveDataPy\n",
    "This notebook receives data from Kafka on the topic 'test', and stores it in the 'time_test' table of Cassandra (created by cassandra_init.script in startup_script.sh).\n",
    "\n",
    "```\n",
    "CREATE KEYSPACE cryptocurrency_market_data WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "CREATE TABLE cryptocurrency_market_data.sent_received(\n",
    " exchange TEXT,\n",
    " cryptocurrency TEXT,\n",
    " basecurrency TEXT,\n",
    " type TEXT,\n",
    " price TEXT,\n",
    " size TEXT,\n",
    " bid TEXT,\n",
    " ask TEXT,\n",
    " open TEXT,\n",
    " high TEXT,\n",
    " low TEXT,\n",
    " volume TEXT,\n",
    " timestamp TEXT,\n",
    "PRIMARY KEY (timestamp)\n",
    ");\n",
    "```\n",
    "\n",
    "A message that gives the crypto_currency_market_data informations is received every second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and start SparkContext\n",
    "Note that SparkContext must be started to effectively load the package dependencies. Two cores are used, since one is needed for running the Kafka receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming satori-volatility\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveToCassandra function\n",
    "Takes a list of tuple (rows) and save to Cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"sent_received\", keyspace=\"cryptocurrency_market_data\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create streaming task\n",
    "* Receive data from Kafka 'test' topic every five seconds\n",
    "* Get stream content, and add receiving time to each message\n",
    "* Save each RDD in the DStream to Cassandra. Also print on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "ssc = StreamingContext(sc, 15)\n",
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'satori-volatility': 1})\n",
    "data = kvs.map(lambda x: json.loads(x[1]))\n",
    "rows= data.map(lambda x:Row(timestamp=x[\"timestamp\"],\n",
    "                            exchange=x[\"exchange\"] if x[\"exchange\"] else 'null',\n",
    "                            cryptocurrency=x[\"cryptocurrency\"] if x[\"cryptocurrency\"] else 'null',\n",
    "                            basecurrency=x[\"basecurrency\"] if x[\"basecurrency\"] else 'null',\n",
    "                            type=x[\"type\"] if x[\"type\"] else 'null',\n",
    "                            price=x[\"price\"] if x[\"price\"] else 'null',\n",
    "                            size=x[\"size\"] if x[\"size\"] else 'null',\n",
    "                            bid=x[\"bid\"] if x[\"bid\"] else 'null',\n",
    "                            ask=x[\"ask\"] if x[\"ask\"] else 'null',\n",
    "                            open=x[\"open\"] if x[\"open\"] else 'null',\n",
    "                            high=x[\"high\"] if x[\"high\"] else 'null',\n",
    "                            low=x[\"low\"] if x[\"low\"] else 'null',\n",
    "                            volume=x[\"volume\"] if x[\"volume\"] else 'null'))\n",
    "                            # https://stackoverflow.com/questions/40713693/inserting-null-values-into-cassandra\n",
    "def formula(price1, price2):\n",
    "    return np.log(float(price1)/float(price2))\n",
    "\n",
    "volatility = data.map(lambda x : (\"volatility 5\", 1/(2*np.log(4))*(np.log(float(x['high'])/float(x['low']))**2),\n",
    "\"volatility 8\", 0.511*((formula(x['high'],x['open'])-formula(x['low'],x['open']))**2)\n",
    "                -0.019*(formula(x['open'],x['open'])*(formula(x['high'],x['open'])+formula(x['low'],x['open']))-2*formula(x['high'],x['open'])*formula(x['low'],x['open']))\n",
    "                -0.383*(formula(x['open'],x['open'])**2)))\n",
    "\"\"\"\n",
    "with close\n",
    "0.511*((formula(x['high'],x['open'])-formula(x['low'],x['open']))**2)\n",
    "-0.019*(formula(x['close'],x['open'])*(formula(x['high'],x['open'])+formula(x['low'],x['open']))-2*formula(x['high'],x['open'])*formula(x['low'],x['open']))\n",
    "-0.383*(formula(x['close'],x['open'])**2)))\n",
    "\"\"\"\n",
    "                                  \n",
    "rows.foreachRDD(saveToCassandra)\n",
    "rows.pprint()\n",
    "volatility.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=sqlContext.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"sent_received\", keyspace=\"cryptocurrency_market_data\")\\\n",
    "    .load()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.registerTempTable(\"sent_received\");\n",
    "data.printSchema()\n",
    "data=sqlContext.sql(\"select * from sent_received\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
